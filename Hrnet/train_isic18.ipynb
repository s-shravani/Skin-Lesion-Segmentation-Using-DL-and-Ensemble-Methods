{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e6ec5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISIC18 Dataset loaded\n",
      "dataset Normalized\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate  (None, 256, 256, 67)         0         ['input_2[0][0]',             \n",
      " )                                                                   'conv2d_19[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, 256, 256, 64)         38656     ['concatenate_8[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['conv2d_20[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate  (None, 128, 128, 192)        0         ['max_pooling2d_4[0][0]',     \n",
      " )                                                                   'conv2d_21[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)          (None, 128, 128, 128)        221312    ['concatenate_9[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['conv2d_22[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenat  (None, 64, 64, 384)          0         ['max_pooling2d_5[0][0]',     \n",
      " e)                                                                  'conv2d_23[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)          (None, 64, 64, 256)          884992    ['concatenate_10[0][0]']      \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['conv2d_24[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenat  (None, 32, 32, 768)          0         ['max_pooling2d_6[0][0]',     \n",
      " e)                                                                  'conv2d_25[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)          (None, 32, 32, 512)          3539456   ['concatenate_11[0][0]']      \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['conv2d_26[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)          (None, 16, 16, 512)          4719104   ['conv2d_27[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2D  (None, 32, 32, 256)          524544    ['conv2d_28[0][0]']           \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenat  (None, 32, 32, 768)          0         ['conv2d_transpose_4[0][0]',  \n",
      " e)                                                                  'conv2d_26[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)          (None, 32, 32, 512)          3539456   ['concatenate_12[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['conv2d_29[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_5 (Conv2D  (None, 64, 64, 128)          262272    ['conv2d_30[0][0]']           \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenat  (None, 64, 64, 384)          0         ['conv2d_transpose_5[0][0]',  \n",
      " e)                                                                  'conv2d_24[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)          (None, 64, 64, 256)          884992    ['concatenate_13[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['conv2d_31[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_6 (Conv2D  (None, 128, 128, 64)         65600     ['conv2d_32[0][0]']           \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenat  (None, 128, 128, 192)        0         ['conv2d_transpose_6[0][0]',  \n",
      " e)                                                                  'conv2d_22[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)          (None, 128, 128, 128)        221312    ['concatenate_14[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['conv2d_33[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_7 (Conv2D  (None, 256, 256, 32)         16416     ['conv2d_34[0][0]']           \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenat  (None, 256, 256, 96)         0         ['conv2d_transpose_7[0][0]',  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " e)                                                                  'conv2d_20[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)          (None, 256, 256, 64)         55360     ['concatenate_15[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['conv2d_35[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['conv2d_36[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24378529 (93.00 MB)\n",
      "Trainable params: 24378529 (93.00 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.2632INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 7s 7s/step - loss: 7.2632 - val_loss: 30.6638 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 29.1174 - val_loss: 155.5737 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 188.8759INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step - loss: 188.8759 - val_loss: 2.1283 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.7377INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 7s 7s/step - loss: 2.7377 - val_loss: 0.9083 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2210INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 7s 7s/step - loss: 1.2210 - val_loss: 0.8433 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7031 - val_loss: 1.5551 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2499INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 8s 8s/step - loss: 1.2499 - val_loss: 0.3757 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3339INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 7s 7s/step - loss: 0.3339 - val_loss: 0.2497 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3131INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 7s 7s/step - loss: 0.3131 - val_loss: 0.2482 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3107INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 7s 7s/step - loss: 0.3107 - val_loss: 0.2317 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2876 - val_loss: 0.3396 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3094 - val_loss: 0.2509 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2686INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 8s 8s/step - loss: 0.2686 - val_loss: 0.1820 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3062INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 7s 7s/step - loss: 0.3062 - val_loss: 0.1638 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2968 - val_loss: 0.1708 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2452 - val_loss: 0.4488 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3404INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: weight_isic18/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step - loss: 0.3404 - val_loss: 0.1333 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2469 - val_loss: 0.1516 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2840 - val_loss: 0.1549 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2670 - val_loss: 0.2899 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2631 - val_loss: 0.2615 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2503 - val_loss: 0.1479 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2436 - val_loss: 0.1351 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2468\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2468 - val_loss: 0.1697 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2250 - val_loss: 0.1769 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2247 - val_loss: 0.1831 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2245 - val_loss: 0.1865 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2242 - val_loss: 0.1863 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2235 - val_loss: 0.1827 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2226 - val_loss: 0.1770 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2216\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2216 - val_loss: 0.1706 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2207 - val_loss: 0.1700 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2207 - val_loss: 0.1695 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2206 - val_loss: 0.1689 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2205 - val_loss: 0.1684 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2204 - val_loss: 0.1680 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2204 - val_loss: 0.1676 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2203\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2203 - val_loss: 0.1672 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2202 - val_loss: 0.1672 - lr: 1.0000e-06\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2202 - val_loss: 0.1672 - lr: 1.0000e-06\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2202 - val_loss: 0.1671 - lr: 1.0000e-06\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2202 - val_loss: 0.1671 - lr: 1.0000e-06\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2202 - val_loss: 0.1671 - lr: 1.0000e-06\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2202 - val_loss: 0.1671 - lr: 1.0000e-06\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2202\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2202 - val_loss: 0.1671 - lr: 1.0000e-06\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2202 - val_loss: 0.1671 - lr: 1.0000e-07\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-07\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-07\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-07\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-07\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-07\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2201\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-07\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-08\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-08\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-08\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-08\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-08\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-08\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2201\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-08\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-09\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-09\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-09\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-09\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-09\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-09\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2201\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-09\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-10\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-10\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-10\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-10\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-10\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-10\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2201\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-10\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-11\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-11\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-11\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-11\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-11\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-11\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2201\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-11\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-12\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-12\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-12\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-12\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-12\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-12\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2201\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-12\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-13\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-13\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-13\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-13\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-13\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-13\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2201\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-13\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-14\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-14\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-14\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-14\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-14\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - val_loss: 0.1671 - lr: 1.0000e-14\n",
      "Trained model saved\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "import pickle\n",
    "\n",
    "base_path = r\"/Users/shravanisajekar/Downloads/27-3-2024 UPDATE\"\n",
    "\n",
    "def BCDU_net_D3(input_tensor=None):\n",
    "    if input_tensor is None:\n",
    "        inputs = Input(shape=(256, 256, 3))\n",
    "    else:\n",
    "        inputs = input_tensor\n",
    "\n",
    "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conc1 = concatenate([inputs, conv1], axis=3)\n",
    "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(conc1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conc2 = concatenate([pool1, conv2], axis=3)\n",
    "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conc2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conc3 = concatenate([pool2, conv3], axis=3)\n",
    "    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(conc3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conc4 = concatenate([pool3, conv4], axis=3)\n",
    "    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(conc4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(pool4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(512, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv10])\n",
    "\n",
    "    return model\n",
    "\n",
    "def dataset_normalized(imgs):\n",
    "    imgs_normalized = np.empty(imgs.shape)\n",
    "    imgs_std = np.std(imgs)\n",
    "    imgs_mean = np.mean(imgs)\n",
    "    imgs_normalized = (imgs-imgs_mean)/imgs_std\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_normalized[i] = ((imgs_normalized[i] - np.min(imgs_normalized[i])) / (np.max(imgs_normalized[i])-np.min(imgs_normalized[i])))*255\n",
    "    return imgs_normalized\n",
    "\n",
    "tr_data = np.load(os.path.join(base_path, 'data_train.npy'))\n",
    "te_data = np.load(os.path.join(base_path, 'data_test.npy'))\n",
    "val_data = np.load(os.path.join(base_path, 'data_val.npy'))\n",
    "\n",
    "tr_mask = np.load(os.path.join(base_path, 'mask_train.npy'))\n",
    "te_mask = np.load(os.path.join(base_path, 'mask_test.npy'))\n",
    "val_mask = np.load(os.path.join(base_path, 'mask_val.npy'))\n",
    "\n",
    "tr_mask = np.expand_dims(tr_mask, axis=3)\n",
    "te_mask = np.expand_dims(te_mask, axis=3)\n",
    "val_mask = np.expand_dims(val_mask, axis=3)\n",
    "\n",
    "print('ISIC18 Dataset loaded')\n",
    "\n",
    "tr_data = dataset_normalized(tr_data)\n",
    "te_data = dataset_normalized(te_data)\n",
    "val_data = dataset_normalized(val_data)\n",
    "\n",
    "tr_mask = tr_mask /255.\n",
    "te_mask = te_mask /255.\n",
    "val_mask = val_mask /255.\n",
    "\n",
    "print('dataset Normalized')\n",
    "\n",
    "input_tensor = Input(shape=(256, 256, 3))\n",
    "model = BCDU_net_D3(input_tensor=input_tensor)\n",
    "model.summary()\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "\n",
    "print('Training')\n",
    "batch_size = 10\n",
    "nb_epoch = 100\n",
    "\n",
    "mcp_save = ModelCheckpoint('weight_isic18', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')\n",
    "\n",
    "history = model.fit(tr_data, tr_mask,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    verbose=1,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(val_data, val_mask),\n",
    "                    callbacks=[mcp_save, reduce_lr_loss])\n",
    "print('Trained model saved')\n",
    "with open('hist_isic18', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1947f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
